- name: Retrieve the current time in order to timestamp files
  ansible.builtin.setup:
    gather_subset:
     - date_time

- name: Ensure .kube directory exists
  ansible.builtin.file:
    path: "~/.kube"
    state: directory
    mode: '0755'

- name: Determine if ~/.kube/config already exists
  ansible.builtin.stat:
    path: ~/.kube/config
  register: existing_kubeconfig

- name: create backup of existing ~/.kube/config
  ansible.builtin.copy:
    src: ~/.kube/config
    dest: "~/.kube/config.{{ ansible_date_time.iso8601_basic_short }}"
  when: 
    - existing_kubeconfig.stat.exists
    - download_kubeconfig

- name: Delete existing ~/.kube/config
  ansible.builtin.file:
    path: ~/.kube/config
    state: absent
  when: 
    - existing_kubeconfig.stat.exists
    - download_kubeconfig

# Check for existing eks cluster
- name: Get list of all EKS clusters
  ansible.builtin.shell:
    cmd: "aws eks list-clusters --region={{ EKS_CLUSTER_REGION }} --output=json"
  register: eks_clusters

# - name: Debug - Show the list of clusters
#   ansible.builtin.debug:
#     var: eks_clusters.stdout

- name: Set fact for list of clusters
  ansible.builtin.set_fact:
    clusters_list: "{{ eks_clusters.stdout | from_json | dict2items | map(attribute='value') | first }}"

- name: Check if the desired cluster exists
  ansible.builtin.set_fact:
    cluster_exists: "{{ EKS_CLUSTER_NAME in clusters_list }}"

# - name: Debug - Show if the desired cluster exists
#   ansible.builtin.debug:
#     msg: "Cluster exists: {{ cluster_exists }}"

# If EKS_CLUSTER_NAME does not exist, set up new eks cluster
- name: "If target cluster {{ EKS_CLUSTER_NAME }} does not exist, create it"
  block: 

    - name: Ensure that the eks_deploy directory exists
      ansible.builtin.file:
        path: "{{ playbook_dir }}/../ascender_install_artifacts/eks_deploy"
        state: directory

    - name: Copy EKS Terraform Directory
      ansible.builtin.copy:
        src: files/eks_deploy
        dest: "{{ playbook_dir }}/../ascender_install_artifacts"
        mode: 0777 

    - name: Initialize Terraform
      ansible.builtin.command:
        cmd: terraform init
        chdir: "{{ playbook_dir }}/../ascender_install_artifacts/eks_deploy"
    
    - name: Terraform Plan
      ansible.builtin.command:
        cmd: terraform plan
        chdir: "{{ playbook_dir }}/../ascender_install_artifacts/eks_deploy"

    - name: Provision EKS cluster via Terraform
      community.general.terraform:
        project_path: "{{ playbook_dir }}/../ascender_install_artifacts/eks_deploy"
        state: present
        variables:
          # home_dir: "/home/{{ ansible_user}}"
          eks_cluster_name: "{{ EKS_CLUSTER_NAME }}"
          kubernetes_version: "{{ EKS_K8S_VERSION }}"
          region: "{{ EKS_CLUSTER_REGION }}"
          num_nodes: "{{ EKS_NUM_WORKER_NODES }}"
          aws_vm_size: "{{ EKS_INSTANCE_TYPE }}"
          volume_size: "{{ EKS_WORKER_VOLUME_SIZE }}"
      register: terraform_output
  
  when: not cluster_exists and EKS_CLUSTER_STATUS=="provision"


- name: Ensure kubeconfig exists
  ansible.builtin.file:
    path: "/home/{{ ansible_user }}/.kube/config"
    state: touch
    owner: "{{ ansible_user }}"
    mode: '0644'
  when: 
    - download_kubeconfig

- name: Retrieve kubeconfig for the EKS cluster
  ansible.builtin.shell:
    cmd: "aws eks update-kubeconfig --region {{ EKS_CLUSTER_REGION }} --name {{ EKS_CLUSTER_NAME }}"
  register: get_credentials_result
  environment:
    KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
  when: 
    - download_kubeconfig

- name: Ensure that the KUBECONFIG file can successfully authenticate to the EKS cluster
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Node
  register: result
  ignore_errors: yes
  retries: 5
  delay: 10
  until: result.resources is defined and result.resources | length > 0

- name: Fail the playbook if EKS authentication check is unsuccessful
  ansible.builtin.fail:
    msg: "Unable to retrieve EKS nodes after multiple retries."
  when: result.resources is not defined or result.resources | length == 0

# Ensure an IAM OIDC provider exists for it, and apply AmazonEBSCSIDriverPolicy
- name: "Apply AmazonEBSCSIDriverPolicy to {{ EKS_CLUSTER_NAME }}"
  block:   

    # - name: Debug - Show if the desired cluster exists
    #   ansible.builtin.debug:
    #     msg: "Applying configurations"

    - name: Get node group name
      ansible.builtin.shell: aws eks list-nodegroups --region {{ EKS_CLUSTER_REGION }} --cluster-name {{ EKS_CLUSTER_NAME }} --query 'nodegroups[0]' --output text
      register: nodegroup_name

    # - name: Debug - nodegroup_name
    #   ansible.builtin.debug:
    #     var: nodegroup_name  

    - name: Get IAM role of EKS worker nodes
      ansible.builtin.shell: aws eks describe-nodegroup --region {{ EKS_CLUSTER_REGION }} --cluster-name {{ EKS_CLUSTER_NAME }} --nodegroup-name {{ nodegroup_name.stdout }} --query 'nodegroup.nodeRole' --output text
      register: node_role_arn

    # - name: Debug - node_role_arn
    #   ansible.builtin.debug:
    #     msg: "{{ node_role_arn.stdout.split('/')[1] }}"

    - name: Define worker node required managed policies
      set_fact:
        new_policies:
          - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
          - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
          - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy

    - name: Get current IAM role managed policies
      amazon.aws.iam_role_info:
        name: "{{ node_role_arn.stdout.split('/')[1] }}"
      register: role_info

    # - name: Debug - node_role_arn
    #   ansible.builtin.debug:
    #     var: role_info

    - name: Extract managed policy ARNs
      ansible.builtin.set_fact:
        existing_policies: "{{ role_info.iam_roles[0].managed_policies | map(attribute='policy_arn') | list }}"

    # - name: Debug - node_role_arn
    #   ansible.builtin.debug:
    #    var: existing_policies

    - name: Combine existing and new managed policies
      ansible.builtin.set_fact:
        combined_policies: "{{ existing_policies | union(new_policies) }}"

    # - name: Debug - node_role_arn
    #   ansible.builtin.debug:
    #    var: combined_policies

    - name: Attach policies to IAM role
      amazon.aws.iam_role:
        name: "{{ node_role_arn.stdout.split('/')[1] }}"
        assume_role_policy_document: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "eks.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
          }
        state: present
        purge_policies: true
        managed_policies: "{{ combined_policies }}"

    - name: Determine the OIDC issuer ID for your cluster
      ansible.builtin.shell:
        cmd: "aws eks describe-cluster --region {{ EKS_CLUSTER_REGION }} --name {{ EKS_CLUSTER_NAME }} --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5"
      register: oidc_query

    # - name: Print out OIDC issuer
    #   ansible.builtin.debug: 
    #     var: oidc_query

    - name: Determine whether an IAM OIDC provider with your cluster's issuer ID is already in your account
      ansible.builtin.shell:
        cmd: "aws iam list-open-id-connect-providers | grep {{ oidc_query.stdout }} | cut -d \"/\" -f4"
      register: oidc_provider_query

    # - name: Print out OIDC identity provider
    #   ansible.builtin.debug: 
    #     var: oidc_provider_query

    - name: Create an IAM OIDC identity provider for your cluster if one does not exist
      ansible.builtin.shell:
        cmd: "eksctl utils associate-iam-oidc-provider --region {{ EKS_CLUSTER_REGION }} --cluster {{ EKS_CLUSTER_NAME }} --approve"
      when: oidc_provider_query.stdout == ""

    - name: Create IAM service account for EBS CSI Driver
      command: eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --region {{ EKS_CLUSTER_REGION }} --cluster {{ EKS_CLUSTER_NAME }} --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --override-existing-serviceaccounts

    - name: Deploy Amazon EBS CSI Driver
      command: kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"    

  when: "(not cluster_exists and EKS_CLUSTER_STATUS == 'provision') or EKS_CLUSTER_STATUS == 'configure'"